{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets, models\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "acc_list = []\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "#         for phase in ['val', 'train']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "#                     print('outputs:', outputs)\n",
    "#                     print('labels:', labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "#                     print('preds:', preds)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "#                 print(\"running_loss:{}, running_corrects:{}\".format(running_loss, running_corrects))\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            acc_list.append(epoch_acc)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "\n",
    "print(\"start\")\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "        transforms.Normalize(mean=[0.453, 0.451, 0.431],\n",
    "                             std=[0.251, 0.243, 0.221])\n",
    "    ])\n",
    "test_data_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "artists_dataset = datasets.ImageFolder(root='/home/lab/yasuhiro/Downloads/best-artworks-of-all-time/images/images',\n",
    "                                           transform=data_transform)\n",
    "dataset_loader = torch.utils.data.DataLoader(artists_dataset,\n",
    "                                             batch_size=4, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root='/home/lab/yasuhiro/Downloads/best-artworks-of-all-time/test',\n",
    "                                           transform=test_data_transform)\n",
    "\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                             batch_size=4, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "        transforms.Normalize([0.453, 0.451, 0.431], [0.251, 0.243, 0.221])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "        transforms.Normalize([0.453, 0.451, 0.431], [0.251, 0.243, 0.221])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {'train':artists_dataset ,'val':test_dataset}\n",
    "dataloaders = {'train':dataset_loader ,'val':test_dataset_loader}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "class_names2 = image_datasets['val'].classes\n",
    "print('classname:', class_names)\n",
    "print('classname2:', class_names2)\n",
    "print(\"Dataloader OK\")\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device OK\")\n",
    "\n",
    "\n",
    "def get_std(dataloader):    \n",
    "    train = iter(dataloader).next()[0]   \n",
    "    mean = np.mean(train.numpy(), axis=(0,2,3))\n",
    "    std = np.std(train.numpy(), axis=(0,2,3))\n",
    "    return mean,std\n",
    "\n",
    "# Get a batch of training data\n",
    "print(\"start iter\")\n",
    "# inputs, classes = next(iter(dataset_loader))\n",
    "print(\"make grid\")\n",
    "# Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[artists_dataset.classes[x] for x in classes])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"start fit model\")\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 50)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "print(\"model OK\")\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)\n",
    "\n",
    "visualize_model(model_ft)\n",
    "print(\"acc_list:\", acc_list)\n",
    "torch.save(model_ft, './resnet50_acc77.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "model = model_ft\n",
    "nb_classes = 50\n",
    "\n",
    "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "print(confusion_matrix)\n",
    "seaborn.heatmap(np.array(confusion_matrix), cmap='Blues')\n",
    "plt.show()\n",
    "# To get the per-class accuracy:\n",
    "print(confusion_matrix.diag()/confusion_matrix.sum(1))\n",
    "\n",
    "confusion_matrix = np.array(confusion_matrix)        \n",
    "print(confusion_matrix)\n",
    "print(confusion_matrix.size)\n",
    "print(confusion_matrix[0].size)\n",
    "for i in range(50):\n",
    "    for j in  range(50):\n",
    "        confusion_matrix[i][j] = confusion_matrix[i][j] / np.sum(confusion_matrix[i])\n",
    "\n",
    "print(confusion_matrix)\n",
    "seaborn.heatmap(confusion_matrix, cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for i in range(len(acc_list)):\n",
    "    if i % 2 == 0:\n",
    "        train_acc.append(acc_list[i].item())\n",
    "    else:\n",
    "        test_acc.append(acc_list[i].item())\n",
    "plt.figure()\n",
    "plt.plot(range(25), train_acc)\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(range(25), test_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import cv2\n",
    "# networks such as googlenet, resnet, densenet already use global average pooling at the end, so CAM could be used directly.\n",
    "\n",
    "net = model_ft\n",
    "finalconv_name = 'layer4'\n",
    "\n",
    "net.eval()\n",
    "\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "# print('params:', params)\n",
    "print('weight_s:', weight_softmax)\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((224,224)),\n",
    "   transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def show_img(path):\n",
    "    img_pil = Image.open(path)\n",
    "\n",
    "    img_tensor = preprocess(img_pil)\n",
    "    img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "    img_variable = img_variable.to(device)\n",
    "    logit = net(img_variable)\n",
    "\n",
    "    # download the imagenet category list\n",
    "    classes = class_names\n",
    "\n",
    "    h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    probs = probs.cpu().numpy()\n",
    "    idx = idx.cpu().numpy()\n",
    "\n",
    "    print('idx', idx)\n",
    "    # output the prediction\n",
    "    for i in range(0, 5):\n",
    "        print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "    # generate class activation mapping for the top1 prediction\n",
    "    CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "    # render the CAM and output\n",
    "    print('output {} for the top1 prediction: {}'.format(path ,classes[idx[0]]))\n",
    "    img = cv2.imread(path)\n",
    "    height, width, _ = img.shape\n",
    "    heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "    result = heatmap * 0.3 + img * 0.5\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(img, alpha=0.9)\n",
    "    plt.imshow(heatmap, alpha=0.5)\n",
    "#     plt.savefig(\"/home/lab/yasuhiro/Downloads/best-artworks-of-all-time/result/\"+path[-13:-4]+'_r50.jpg')\n",
    "    plt.show()\n",
    "    \n",
    "dir = \"/home/lab/yasuhiro/Downloads/best-artworks-of-all-time/\"\n",
    "list_labels = os.listdir(os.path.join(dir, \"test\"))\n",
    "for i in list_labels:\n",
    "    list_image = os.listdir(os.path.join(dir, \"test\", i))\n",
    "    for j in list_image:\n",
    "        show_img(os.path.join(dir, \"test\", i, j))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda9.2]",
   "language": "python",
   "name": "conda-env-cuda9.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
